
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a Master‚Äôs student graduate from EPFL, where I have carried out some research projects on applied machine learning:\nEmploying autoencoders to predict stellar properties at the astrophysics laboratory. Exploring computer vision methods for image registration of a fly‚Äôs neuron activations in Pavan Ramdya‚Äôs Lab. During my thesis at IBM Research in Z√ºrich, I applied NLP methods, uncertainty quantification (UQ), and explored explainable AI techniques to quantify chemical sustainability under the supervision of Prof. Martin Jaggi and Dr. Amol Thakkar. You can read more here. I enjoy hiking ‚õ∞, playing the piano üéπ, watching old movies üìΩ, and learning both about philosophy and the newest tech!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Master‚Äôs student graduate from EPFL, where I have carried out some research projects on applied machine learning:\nEmploying autoencoders to predict stellar properties at the astrophysics laboratory. Exploring computer vision methods for image registration of a fly‚Äôs neuron activations in Pavan Ramdya‚Äôs Lab.","tags":null,"title":"Adri√°n Sager La Ganga","type":"authors"},{"authors":[],"categories":[],"content":" NumPy‚Äôs numpy.random.choice() method samples items at random from an array.\nUnfortunately, sampling without replacement an int from $0$ to $N-1$ requires $O(N)$ memory. Thus, the following code will result in an OOM error (if not completely crash your machine):\nimport numpy as np N = 1_000_000_000_000 np.random.choice(N, size=2, replace=False) This is because numpy.random.choice() creates (at least up to NumPy version 1.24) an array of size $N$ (in our example, ~465 GB !), as it can be seen in the source code:\ndef choice(self, a, size=None, replace=True, p=None): ... # pop_size == a when `a` is an `int` idx = self.permutation(pop_size)[:size] ... if a.ndim == 0: return idx def permutation(self, object x): ... if isinstance(x, (int, np.integer)): # this creates an array of size `x` arr = np.arange(x) self.shuffle(arr) return arr However, intuitively we shouldn‚Äôt need to create an array of size $N$ if we are sampling a small amount of items $k$, right?\nWe could simply keep sampling with replacement (which doesn‚Äôt require $O(N)$ memory) until we have an array of $k$ different items.\nThis very basic and straightforward algorithm would be defined as follows:\nAlgorithm 1 An algorithm to draw without replacement. $k\u0026gt;0$ items are taken without replacement from a universe of size $N$. $choice(N, k)$ samples $k$ items with replacement with a space complexity of $O(k \\log{N})$: Require: $N \\geq (2 + \\sqrt{2}) \\cdot k$ 1: $X \\leftarrow \\{choice(N,1)\\}$ 2: while $\\vert X\\vert \\neq k$ do 3: $s \\leftarrow choice(N, 1)$ 4: if $s \\notin X$ then 5: $X \\leftarrow X \\cup \\left\\{ s \\right\\}$ 6: end if 7: end while Which in code looks like,\ndef choice(N: int, k: int = 1, replace: bool = True, p=None): \u0026#34;\u0026#34;\u0026#34;Sample `k` elements from 0 to `N-1` with or without replacement.\u0026#34;\u0026#34;\u0026#34; if not replace and N \u0026gt;= 3.414213562373095 * k: X = [np.random.choice(N, size=1, replace=True, p=p)] X = set(X) while len(X) != k: s = np.random.choice(N, size=1, replace=True, p=p) X.add(s) return X else: return np.random.choice(N, size=k, replace=replace, p=p) But why require that $N \\geq (2 + \\sqrt{2})\\cdot k$? We will figure it out by proving a couple of theorems:\nTheorem 1‚ÄÇ Algorithm 1 has a space complexity of $O(k \\log{N})$. Proof. Both $X$ and $choice(N, k)$ occupy $O(k \\log{N})$ space, since $\\vert X\\vert$ is at most $k$ and our largest number, $N$, requires $O(\\log{N})$ bits to be stored. $\\square$ This one was very simple. But, how about time complexity?\nTheorem 2‚ÄÇ Algorithm 1 has an expected time complexity of $O(k)$, i.e., it has a time complexity of $\\Theta(k)$. Proof. In order to advance in the while loop, we need to add new elements to $X$. Thus: $$ \\begin{align} P\\left[\\text{sampling unique item after }i+1\\text{ draws}\\right] \u0026amp;= \\left(\\frac{\\vert X\\vert}{N}\\right)^i \\cdot \\left( 1 - \\frac{\\vert X\\vert}{N} \\right) \\\\ \u0026amp;\\leq \\left(\\frac{k-1}{N}\\right)^i \\cdot 1 \\end{align} $$ Using this probability bound, the expected number of items drawn is: $$ \\begin{align} \\mathbb{E}[\\text{# of items drawn}] \\leq 1 \u0026amp;+ \\mathbb{E}[\\text{# of draws to increment }\\vert X\\vert=1\\text{ by 1}] \\\\ \u0026amp;+ \\cdots \\\\ \u0026amp;+ \\mathbb{E}[\\text{# of draws to increment }\\vert X\\vert=(k-1)\\text{ by 1}] \\\\ \\leq 1 \u0026amp;+ (k-1)\\sum_{i=1}^\\infty i\\left(\\frac{k-1}{N}\\right)^{i} \\end{align} $$ Since we have to draw 1 item in line 1 of the algorithm and then we have to draw $(k-1)$ new unique items in the while loop. We now note that the algorithm requires that $N\\geq (2 + \\sqrt{2})\\cdot k$, so: $$ \\begin{align} N \\geq (2 + \\sqrt{2}\u0026amp;)\\cdot k \\geq 2\\cdot k \\newline \\frac{k-1}{N} \u0026amp;\\leq 2^{-1} \\newline \u0026amp;\\downarrow \\newline \\mathbb{E}[\\text{# of items drawn}] \u0026amp;\\leq 1 + (k-1) \\sum_{i=1}^\\infty i\\cdot2^{-i} \\newline \u0026amp;= 1 + (k-1)\\cdot 2 \\newline \u0026amp;= O(k) \\end{align} $$\n$\\square$ Note that sets in Python have $O(1)$ lookup and insert times, required for lines 4 and 5 in the algorithm.\nHowever, this is not all. We have used the fact that $N \\geq 2k$ in our proof, but now let‚Äôs generalize to $N \\geq ak$ with $a \u0026gt; 1$. Note that $a=1$ would include the possibility that $N=k$, in which we just sample all items in the universe of size $N$, and this is not interesting for our purposes. Also, $a\u0026lt;1$ includes cases where $N\u0026lt;k$, which is impossible and our algorithm would not halt.\nLemma 1‚ÄÇ If $N\\geq ak$ for $a\u0026gt;1$, the expected number of items drawn is less than $1 + (k-1)\\cdot a/(a-1)^2$. Proof. $N\\geq ak$ implies that, $$ \\begin{align} \\frac{k-1}{N} \u0026amp;\\leq a^{-1} \\\\ \u0026amp;\\downarrow \\\\ \\mathbb{E}[\\text{# of items drawn}] \u0026amp;\\leq 1 + (k-1) \\sum_{i=1}^\\infty i\\cdot a^{-i} \\\\ \u0026amp;= 1 + (k-1)\\frac{a}{(a-1)^2} \\end{align} $$ Since, $$ \\sum_{i=1}^\\infty i\\cdot a^{-i} = \\frac{a}{(a-1)^2} \\qquad \\forall a, \\vert a\\vert \u0026gt; 1 $$ $\\square$ With this lemma we can prove the following theorem:\nTheorem 3‚ÄÇ Algorithm 1 draws O(2k) items w.p. (with probability) less than $(k-1)/N$. Proof. We define $Y := (\\text{# of items drawn} - 1)$, which is the amount of items drawn in the while loop. We recall Markov\u0026#39;s inequality: $$ P\\left[ Y \\geq ‚Ä¶","date":1684846970,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684846970,"objectID":"12b16fa226061d693cc07c9f57f8e886","permalink":"https://sager611.github.io/post/memory-efficient-choice-function/","publishdate":"2023-05-23T15:02:50+02:00","relpermalink":"/post/memory-efficient-choice-function/","section":"post","summary":"NumPy‚Äôs numpy.random.choice() method samples items at random from an array.\nUnfortunately, sampling without replacement an int from $0$ to $N-1$ requires $O(N)$ memory. Thus, the following code will result in an OOM error (if not completely crash your machine):","tags":["numpy","complexity","algorithm"],"title":"Memory efficient numpy.random.choice()","type":"post"},{"authors":[],"categories":[],"content":"I recently bought a new laptop with Windows 11 and wanted to migrate my whole hard drive contents from my old laptop with an Arch Linux installation.\nFollow this tutorial if you want to clone your old laptop‚Äôs hard drive to your new laptop‚Äôs hard drive via SSH in the same local network.\nTable of Contents Boot from Live USB Setup SSH Clone hard disk via SSH Boot from Live USB In our new machine we will run all commands from a live USB Arch Linux image.\nFollow the official Arch Linux tutorial to boot an Arch Linux image from an USB.\nOnce you have your live USB, plug it in your new laptop and go to the boot menu.\nIn my case, I have a Thinkpad and I had to press enter and then F12 when powering on the laptop.\nI also had to disable Secure Boot beforehand to boot from my live USB.\nMake sure to connect to the same local wifi network in both machines before continuing.\nSetup SSH The following commands are executed as root user.\nRun su on your old machine to become root user.\nCheck IP address of new machine with ip address:\nroot@archiso ~ # ip address \u0026lt;...\u0026gt; 4: wlan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether a0:59:50:e1:5f:9b brd ff:ff:ff:ff:ff:ff inet 192.168.1.76/24 metric 600 brd 192.168.1.255 scope global dynamic wlan0 valid_lft 39244sec preferred_lft 39244sec inet6 fe80::a259:50ff:fee1:5f9b/64 scope link valid_lft forever preferred_lft forever Our local ip in this example is 192.168.1.76.\nStart ssh service in new machine:\nsystemctl start sshd.service Set the root password with passwd. Simply run passwd in your new machine.\nNow we setup ssh so it doesn‚Äôt ask for password when connecting to our new machine. In your old machine:\nssh-keygen -t ed25519 ssh-copy-id -i .ssh/id_thinkpadp15.pub root@192.168.1.76 In my case it looks like:\n[root@thinkpad-t440s ~]$ ssh-keygen -t ed25519 Generating public/private ed25519 key pair. Enter file in which to save the key (/home/saheru/.ssh/id_ed25519): .ssh/id_thinkpadp15 Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in .ssh/id_thinkpadp15 Your public key has been saved in .ssh/id_thinkpadp15.pub The key fingerprint is: SHA256:DJ74VMCmqQ6nB//xZkPhSmPZRR00lVqskcZ2TH5N5wc saheru@thinkpad-t440s The key\u0026#39;s randomart image is: +--[ED25519 256]--+ | .. ++Bo.E o| | o.. Bo* =.| | +...o *. . +| | oo.=. o . .| | ..++oS | |o o =o+ | | B o.+. | |. + .o+ | | . ..o.. | +----[SHA256]-----+ [root@thinkpad-t440s ~]$ ssh-copy-id -i .ssh/id_thinkpadp15.pub root@192.168.1.76 Note that I named my key .ssh/id_thinkpadp15.\nThe last command will ask for the root password, which is the one we have set in the previous step.\nAppend your new machine‚Äôs IP with the corresponding private key in your old machine‚Äôs .ssh/config (create the file if it doesn‚Äôt exist), in my case:\nHost 192.168.1.76 HostName 192.168.1.76 User root IdentityFile ~/.ssh/id_thinkpadp15 Now ssh won‚Äôt prompt for a password when we connect to our new machine.\nClone hard disk via SSH We will use the dd command from coreutils and ssh to remotely clone our hard disk from our old machine to our new machine.\nCheck the official Arch wiki for more info on cloning with dd.\nThe following commands completely erase the new machine‚Äôs hard disk contents! Make sure to save a backup image. First, make sure you have enough space in your new hard disk. You can check this by running lsblk on your old and new machines.\nAlso make sure your hard disk in your new machine is unmounted with lsblk (MOUNTPOINTS should be empty).\nIn my case the hard disk in my new machine was nvme0n1. Make sure you choose the correct disk by looking at SIZE in lsblk. To avoid corrupting files during cloning, you should run the following command from another live USB in your old machine and make sure your hard drive sda is unmounted.\nRun the following in your old machine to clone the disk:\ndd if=/dev/sda | ssh root@192.168.1.76 dd of=/dev/nvme0n1 status=progress Change the IP, and sda and nvme0n1 according to your source and destination disk names.\nIt will take 5-10h to clone a 512GB SSD. Once it finishes, you will have copied all the contents from your old machine to your new machine, including GRUB and LUKS (also with logical volumes) if you had them setup.\n","date":1678197067,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678197067,"objectID":"d42d1d3d5ad3291d8028253de88d2467","permalink":"https://sager611.github.io/post/clone-arch-disk/","publishdate":"2023-03-07T14:51:07+01:00","relpermalink":"/post/clone-arch-disk/","section":"post","summary":"I recently bought a new laptop with Windows 11 and wanted to migrate my whole hard drive contents from my old laptop with an Arch Linux installation.\nFollow this tutorial if you want to clone your old laptop‚Äôs hard drive to your new laptop‚Äôs hard drive via SSH in the same local network.","tags":[],"title":"Cloning Arch Linux hard disk remotely to new machine","type":"post"},{"authors":[],"categories":[],"content":" In 2020 I was following lectures on automatic control and learned about the transfer function of a system and ways to visualize it, at which point I was curious so I went ahead and wrote a simple visualizer for generic transfer functions.\nYou can jump ahead to the visualizer.\nSetup The basic idea is that we can build a system with a feedback loop that takes an input signal $x(t)$ and outputs another signal $y(t)$:\nWe then focus on linear time-invariant (LTI) transformations, which in a few words are those which take as input $e(t)$ and output $y(t) = (e \\ast h)(t)$, where $h(t)$ is called the impulse response since it is exactly the value of the output $y(t)$ if our input where an impulse $e(t)=\\delta(t)$.\nIn the Laplace function space, the convolution of our transformation becomes a multiplication $Y(s) = H(s)E(s)$.\nLet‚Äôs plug in our $H(s)$ in the feedback loop!\nWe recursively apply it so we end up with:\n$$ \\begin{align*} Y(s) \u0026amp;= \\left( H(s) + H(s)^2 + H(s)^3 + \\dots \\right) \\cdot X(s) \\\\ \u0026amp;= \\frac{H(s)}{1 - H(s)} \\cdot X(s) \\end{align*} $$ Visualizer Note that if our $H(s)$ touches the value 1 we‚Äôll end up with an $\\infty$, oops.\nSo, when designing $H(s)$ we want to make sure that it is some distance away from 1.\nBut $H: \\mathbb{C} \\rightarrow \\mathbb{C}$, how can we visualize it?\nWell, there are multiple approaches and this is where I got curious and decided to write a visualizer in p5.js. I included a cartesian plot and a Nichols plot. Other common plots include Bode and Nyquist.\nI also included a simple abstract syntax tree (AST) parser and traversal so you can write any $H(s)$ you want and see what happens!\nYou can try it out right here:\nControls:\nDrag to move around.\nMouse wheel to zoom in/out.\nSpace to reset offset.\nYou may encounter some bugs if you introduce incorrect\n$H(s)$ and the visualizer may crash. Use with care.\n","date":1677513077,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677513077,"objectID":"dfbdef4f447a336c3440bf8fb1e9da0a","permalink":"https://sager611.github.io/post/p5js-vis-transfer-function/","publishdate":"2023-02-27T16:51:17+01:00","relpermalink":"/post/p5js-vis-transfer-function/","section":"post","summary":"In 2020 I was following lectures on automatic control and learned about the transfer function of a system and ways to visualize it, at which point I was curious so I went ahead and wrote a simple visualizer for generic transfer functions.","tags":["p5js","control theory"],"title":"Visualizing transfer functions in p5.js","type":"post"},{"authors":[],"categories":[],"content":" What if we could systematically quantify how sustainable a reaction is?\nIn this project (my master‚Äôs thesis at EPFL) I explored how useful it would be if we quantified sustainability as how likely it is that a reaction is sustainable. This likelihood would then be computed by employing uncertainty quantification (UQ) on some AI model‚Äôs prediction.\nThe main contribution of the project is an extensible toolkit with sustainability metrics based on AI model predictions.\nIn this page I collect some of my thoughts derived from this project.\nYou can read the full report here.\nThis work was created and funded as part of the NCCR Catalysis Young Talents Fellowship, a National Centre of Competence in Research funded by the Swiss National Science Foundation. Table of Contents AI-metric definition Modelling uncertainty Quantifying AI-metrics Validating AI-metrics Don‚Äôt pay attention to the attention ‚Ä¶ Putting it all together AI-metric definition Let‚Äôs say a chemical reaction can be one of $\\mathcal{C}$ classes, for ex., according to its type: ‚Äúcarboxylic acid to acid chloride‚Äù, ‚Äútranslocase-catalyzed‚Äù, etc.\nSome of these classes are sustainable, $\\mathcal{C}_\\text{sust}$, some are not, $\\mathcal{C}_\\text{non-sust}$, and we may have an unrecognized class, $\\mathcal{C}_\\text{unrec}=\\{y_\\text{unrec}\\}$.\nClick to view the reaction type distribution In reality, our distribution of reaction types is hierarchical, but the argument still applies We can then quantify how likely we expect some reaction $\\pmb{x}$ to be of some class $y\\in\\mathcal{C}$:\n$$ \\text{AI-metric}_y := \\mathbb{E}_{p(\\pmb{\\theta}\\vert\\mathcal{D})}\\left[p(y\\vert\\pmb{x}, \\pmb{\\theta})\\right]\\cdot\\text{confidence}(\\pmb{x}, y) \\quad \\in [0,1]$$ Where $\\pmb{\\theta}$ are the parameters of our AI model after training it on some dataset $\\mathcal{D}$.\nNote how we are scaling by how confident we are that this expected likelihood is correct. In our case,\n$$\\text{confidence}(\\pmb{x}, y) := 1 - 2\\cdot\\text{stddev}_{p(\\pmb{\\theta}\\vert\\mathcal{D})}\\big(p(y\\vert\\pmb{x}, \\pmb{\\theta})\\big)$$ Which is always in $[0,1]$ since probabilities cannot deviate more than $1/2$.\nBut, how do we score how likely our $\\pmb{x}$ is of any class in $\\mathcal{C}_\\text{sust}$?\nWe simply take the maximum over $\\mathcal{C}_\\text{sust}$:\n$$\\text{AI-metric} := \\max_{y\\in\\mathcal{C}_\\text{sust}} \\text{AI-metric}_y \\quad \\in [0, 1] $$ Why not sum over $\\mathcal{C}_\\text{sust}$? Formally, we are asking $p(\\cup_{\\mathcal{C}_\\text{sust}}y\\vert \\pmb{x},\\mathcal{D})$, which is: $$ \\max_{y\\in\\mathcal{C}_\\text{sust}} p(y\\vert\\pmb{x},\\mathcal{D}) \\leq p(\\cup_{\\mathcal{C}_\\text{sust}}y\\vert \\pmb{x},\\mathcal{D}) \\leq \\sum_{y\\in\\mathcal{C}_\\text{sust}} p(y\\vert\\pmb{x},\\mathcal{D}) $$ Since we are estimating our metric (see next section) and since some $\\pmb{x}$ may be of multiple classes, if we sum probabilities we would be overestimating the true value and also our score may end up above 1. It is also crucial that in our case our AI model is single-label, so it is designed to only predict the highest probability class. Modelling uncertainty Without getting too philosophical, we want to believe that for sure there is some underlying true classification set for an input $\\pmb{x}$, $\\mathcal{C}_\\text{true} \\subseteq \\mathcal{C}$, so:\n$$ p(y\\vert\\pmb{x}) = \\begin{cases}1 \u0026amp; \\text{if }y\\in\\mathcal{C}_\\text{true} \\\\ 0 \u0026amp; \\text{o.w.}\\end{cases} $$ Note that (by our philosophy) $p(y\\vert\\pmb{x})$ is equivalent to $p(y\\vert\\pmb{x},\\mathcal{D}_\\text{all})$ where $\\mathcal{D}_\\text{all}$ is all the knowledge of the universe (quite some information!).\nOf course, in reality at best we know:\n$$p(y\\vert\\pmb{x},\\mathcal{D}) = \\mathbb{E}_{p(\\pmb{\\theta}\\vert\\mathcal{D})}\\left[ p(y\\vert\\pmb{x},\\pmb{\\theta}) \\right]$$ For our observed training set $\\mathcal{D}$.\nThis means that we are in the presence of two sources of uncertainty1:\nEpistemic $\\pmb{\\theta}$: due to the limited expresiveness of our model and its optimization procedure. Aleatoric $\\mathcal{D}$: due to noise in our training set (some entries aren‚Äôt reactions or have a wrong label). Quantifying AI-metrics We use efficient uncertainty quantification (UQ) techniques to estimate our AI-metric according to the two sources of uncertainty2:\nMonte Carlo Dropout (MCDropout) for epistemic uncertainty $\\pmb{\\theta}$: We leave our Drouput layers on during inference, so for each forward pass on the same $\\pmb{x}$ we are effectively sampling different submodels from an ensemble.\nTest-time data augmentation for aleatoric uncertainty $\\mathcal{D}$: We augment $\\pmb{x}$ during inference.\nWe then have $N\\times M$ likelihoods $p(y\\vert\\pmb{x}^{(i)}, \\pmb{\\theta}^{(j)})$ where we sampled our input $\\pmb{x}^{(1)},\\dots,\\pmb{x}^{(N)}$ and our model $\\pmb{\\theta}^{(1)},\\dots,\\pmb{\\theta}^{(M)}$. Our AI-metric then simply uses the average and stddev of these values to compute its score.\nFor example,\nTop-5 reaction types for reaction according to the AI-metric. The actual true types for ‚Ä¶","date":1677339429,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677339429,"objectID":"bb373e34aa74a9e0f96d626986715a37","permalink":"https://sager611.github.io/project/epfl-master-thesis/","publishdate":"2023-02-25T16:37:09+01:00","relpermalink":"/project/epfl-master-thesis/","section":"project","summary":"What if we could systematically quantify how sustainable a reaction is?\nIn this project (my master‚Äôs thesis at EPFL) I explored how useful it would be if we quantified sustainability as how likely it is that a reaction is sustainable.","tags":["UQ","NLP","chemistry","sustainability"],"title":"UQ-based metrics for chemical sustainability","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://sager611.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]